---
title: Monitor Redpanda
---

<head>
  <meta name="title" content="Monitor Redpanda | Redpanda Docs" />
  <meta
    name="description"
    content="Metrics to monitor the health of your system to predict issues and optimize performance."
  />
</head>

import Aliases from "@site/docs/shared/_aliases.mdx";

Redpanda exports metrics through Prometheus endpoints for you to use for monitoring system health and optimizing system performance.

A Redpanda node exports public metrics from the `/public_metrics` endpoint through the admin API port (by default, `<node-addr>:9644/public_metrics`). To see descriptions about the available public metrics, query the admin endpoint:

```bash
curl <node-addr>:9644/public_metrics | grep "[HELP|TYPE]"
```

:::note
To maximize monitoring performance by minimizing the cardinality of data, metrics are exported when their underlying features are in use, and are not exported when not in use. For example, a metric for consumer groups, [redpanda_kafka_consumer_group_committed_offset](#redpanda_kafka_consumer_group_committed_offset), is not exported when no groups are registered.
:::

Before v22.2, a Redpanda node provided metrics only through the `/metrics` endpoint (`<node-addr>:9644/metrics`). While this endpoint is still provided by Redpanda, it includes many internal metrics that are unnecessary for a typical Redpanda user to monitor. Consequently, the `/public_metrics` endpoint was added to provide a smaller set of important metrics that can queried and ingested more quickly and inexpensively. 

:::tip
Use `/public_metrics` for your primary dashboards for system health.

Use `/metrics` for detailed analysis and debugging.
:::

To learn more about `/metrics`, see [Internal Metrics](../../reference/internal-metrics).

The rest of this topic covers the following about monitoring `/public_metrics`:

- How to configure Prometheus to monitor Redpanda
- Key metrics to monitor for performance
- Reference list of available metrics 

## Configure Prometheus

[Prometheus](https://prometheus.io/) is a system monitoring and alerting tool. It collects and stores metrics as time-series data identified by a metric name and key/value pairs.

To configure and use Prometheus to capture and create alerts for Redpanda `/public_metrics`, follow these steps:

1. Generate the configuration on an existing Prometheus instance:

    ```bash
    rpk generate prometheus-config
    ```

    :::note

    When you run the command on a node where Redpanda is running, it displays other available nodes. If Redpanda is not running, you can either set the `--seed-addr` flag to specify a remote Redpanda node to discover the additional nodes, or set `--node-addrs` with a comma-separated list of known cluster node addresses. For example, use `--node-addrs`:

        ```bash
        rpk generate prometheus-config --job-name redpanda-metrics-test --node-addrs 172.31.18.239:9644,172.31.18.239:9643,172.31.18.239:9642
        ```

    :::

1. Edit the `prometheus.yml` file in the Prometheus root folder to add the Redpanda configuration under `scrape_configs`. Customize `targets` for the names and number of running nodes.

    ```yaml
    scrape_configs:
    - job_name: redpanda-metrics-test
      static_configs:
      - targets:
        - redpanda-0:9644
        - redpanda-1:9644
        - redpanda-2:9644
    metrics_path: /public_metrics
    ```

1. Save the configuration file, and restart Prometheus to apply changes.

1. Observe in Prometheus that metrics from `/public_metrics` endpoints of your Redpanda cluster are scraped.

## Generate Grafana dashboard

[Grafana](https://grafana.com/oss/grafana/) is a tool to query, visualize, and generate alerts for metrics.

Redpanda supports generating Grafana dashboards from its metrics endpoints with `rpk`. For example, generate a JSON file to import into Grafana as a new dashboard for public_metrics by running `rpk generate grafana-dashboard`: 

```
rpk generate grafana-dashboard \
  --datasource prometheus \
  --metrics-endpoint <node-addr>:9644/public_metrics > redpanda-dashboard.json 
```

See the [rpk generate grafana-dashboard](../reference/rpk/rpk-generate/rpk-generate-grafana-dashboard.mdx) reference for details about the command.

## Monitoring for performance

This section provides guidelines and example metrics queries to optimize your cluster's performance.

:::tip
To help detect and mitigate anomalous system behaviors, capture baseline metrics of your healthy system at different stages (at start-up, under high load, in steady state) so you can set thresholds and alerts according to those baselines.
:::

The links in the chain of 

### Infrastructure resources

A healthy system has sufficient margins to handle processing, storage, and I/O loads.

#### CPU usage

For the total CPU uptime, monitor [redpanda_uptime_seconds_total](#redpanda_uptime_seconds_total) and [redpanda_cpu_busy_seconds_total](#redpanda_cpu_busy_seconds_total):

```
rate(redpanda_uptime_seconds_total[5m])
rate(redpanda_cpu_busy_seconds_total[5m])
```

#### Memory allocated

To monitor the percentage of memory allocated, use a formula with [redpanda_memory_allocated_memory](#redpanda_memory_allocated_memory) and [redpanda_memory_free_memory](#redpanda_memory_free_memory):

```
sum(redpanda_memory_allocated_memory) / (sum(redpanda_memory_free_memory) + sum(redpanda_memory_allocated_memory))
```

#### Disk used

To monitor the percentage of disk consumed, use a formula with [redpanda_storage_disk_free_bytes](#redpanda_storage_disk_free_bytes) and [redpanda_storage_disk_total_bytes](#redpanda_storage_disk_total_bytes): 

```
1 - (sum(redpanda_storage_disk_free_bytes) / sum(redpanda_storage_disk_total_bytes))
```

Monitor also [redpanda_storage_disk_free_space_alert](#redpanda_storage_disk_free_space_alert) for an alert when available disk space is becoming degraded or low.

#### IOPS

For read and write I/O operations per second (IOPS), monitor the [redpanda_io_queue_total_read_ops](#redpanda_io_queue_total_read_ops) and [redpanda_io_queue_total_write_ops](#redpanda_io_queue_total_write_ops) counters: 

```
rate(redpanda_io_queue_total_read_ops[5m]),
rate(redpanda_io_queue_total_write_ops[5m])
```


### Throughput

The total throughput of a cluster can be measured by the producer and consumer rates across all topics.

To observe the total producer and consumer rates of a cluster, monitor [redpanda_kafka_request_bytes_total](#redpanda_kafka_request_bytes_total) with the `produce` and `consume` labels, respectively. 

For example, for the produce rate, create a query to get the produce rate across all topics:

```
sum(rate(redpanda_kafka_request_bytes_total{redpanda_request="produce"} [5m] )) by (redpanda_request)
```

Similarly, for the consume rate, create a query to get the total consume rate across all topics:

```
sum(rate(redpanda_kafka_request_bytes_total{redpanda_request="consume"} [5m] )) by (redpanda_request)
```

When you see abnormal, unhealthy spikes or dips in producer or consumer throughput, look for correlation with changes in the number of active connections ([redpanda_rpc_active_connections](#redpanda_rpc_active_connections)) and logged errors.


### Latency

Latency should be consistent between produce and fetch sides. It should also be consistent over time. Take periodic snapshots and watch out for significant changes over a short duration.

In Redpanda the latency of produce and fetch requests includes the latency of internal inter-node RPCs that are born from Redpanda's internal implementation using Raft.

#### Kafka consumer latency

To monitor Kafka consumer request latency, use the [redpanda_kafka_request_latency_seconds_bucket](#redpanda_kafka_request_latency_seconds_bucket) histogram with the label `redpanda_request="consume"`. For example, create a query for the 99th percentile:

```
histogram_quantile(0.99, sum(rate(redpanda_kafka_request_latency_seconds_bucket{redpanda_request="consume"}[5m])) by (le, provider, region, instance, namespace, pod))
```

The rate of Kafka consumer requests can be monitored with `redpanda_kafka_request_latency_seconds_count` with the `redpanda_request="consume"` label:

```
rate(redpanda_kafka_request_latency_seconds_count{redpanda_request="consume"}[5m])
```

#### Kafka producer latency

To monitor Kafka producer request latency, use the [redpanda_kafka_request_latency_seconds_bucket](#redpanda_kafka_request_latency_seconds_bucket) histogram with the `redpanda_request="produce"` label. For example, create a query for the 99th percentile:

```
histogram_quantile(0.99, sum(rate(redpanda_kafka_request_latency_seconds_bucket{redpanda_request="produce"}[5m])) by (le, provider, region, instance, namespace, pod))
```

The rate of Kafka producer requests can be monitored with `redpanda_kafka_request_latency_seconds_count` with the `redpanda_request="produce"` label:

```
rate(redpanda_kafka_request_latency_seconds_count{redpanda_request="produce"}[5m])
```

#### Internal RPC latency

To monitor Redpanda internal RPC latency, use the [redpanda_rpc_request_latency_seconds](#redpanda_rpc_request_latency_seconds) histogram. For example, create a query for the 99th percentile latency:   

```
histogram_quantile(0.99, (sum(rate(redpanda_rpc_request_latency_seconds_bucket[5m])) by (le, provider, region, instance, namespace, pod, redpanda_server)))
```

The rate of internal RPC requests can be monitored with `redpanda_rpc_request_latency_seconds_count`:

```
rate(redpanda_rpc_request_latency_seconds_count[5m])
```


### Partition health

#### Leadership changes

Stable clusters have a consistent balance of leaders across all nodes, with few to no leadership transfers between nodes.

To observe changes in leadership, monitor the [redpanda_raft_leadership_changes](#redpanda_raft_leadership_changes) counter. For example, use a query to get the total rate of increase of leadership changes for a cluster:

```
sum(rate(redpanda_raft_leadership_changes{redpanda_cloud_data_cluster_name=<cluster-name>}[5m]))
```
 
#### Under-replicated partitions

A healthy cluster has partition data fully replicated across its brokers. 

An under-replicated partition is at higher risk of data loss. It also adds latency because messages must be replicated before being committed. To know when a partition isn't fully replicated, create an alert for the [redpanda_kafka_under_replicated_replicas](#redpanda_kafka_under_replicated_replicas) gauge when it is greater than zero:

```
redpanda_kafka_under_replicated_replicas > 0
```

Under replication can be caused by unresponsive brokers. When an alert on `redpanda_kafka_under_replicated_replicas` is triggered, identify the problem broker(s) and examine their log(s). 

#### Leaderless partitions

A healthy cluster has a leader for every partition. 

A partition without a leader cannot exchange messages with producers or consumers. To know when a partition doesn't have a leader, create an alert for the [redpanda_cluster_unavailable_partitions](#redpanda_cluster_unavailable_partitions) gauge when it is greater than zero: 

```
redpanda_cluster_unavailable_partitions > 0
```

Leaderless partitions can be caused by unresponsive brokers. When an alert on `redpanda_cluster_unavailable_partitions` is triggered, identify the problem broker(s) and examine their log(s). 


### Services

#### Schema registry

Schema Registry request latency

```
histogram_quantile(0.99, (sum(rate(redpanda_schema_registry_request_latency_seconds_bucket[5m])) by (le, provider, region, instance, namespace, pod)))
```

Schema Registry request rate

```
rate(redpanda_schema_registry_request_latency_seconds_count[5m]) + sum without(redpanda_status)(rate(redpanda_schema_registry_request_errors_total[5m]))
```

Schema Registry request error rate

```
rate(redpanda_schema_registry_request_errors_total[5m])
```

#### REST proxy

REST proxy request latency

```    
histogram_quantile(0.99, (sum(rate(redpanda_rest_proxy_request_latency_seconds_bucket[5m])) by (le, provider, region, instance, namespace, pod)))
```

REST proxy request rate

```
rate(redpanda_rest_proxy_request_latency_seconds_count[5m]) + sum without(redpanda_status)(rate(redpanda_rest_proxy_request_errors_total[5m]))
```

REST proxy request error rate

```
rate(redpanda_rest_proxy_request_errors_total[5m])
```

### Consumer groups

When working with Kafka consumer groups, the consumer group lag--the difference between the broker's latest (max) offset and the group's last committed offset--is a performance indicator of how fresh, or real-time, is the data being consumed.

To monitor consumer group lag, create a query with the [redpanda_kafka_max_offset](#redpanda_kafka_max_offset) and [redpanda_kafka_consumer_group_committed_offset](redpanda_kafka_consumer_group_committed_offset) gauges:

```
max by(redpanda_namespace, redpanda_topic, redpanda_partition)(redpanda_kafka_max_offset{redpanda_namespace="kafka"}) - on(redpanda_topic, redpanda_partition) group_right max by(redpanda_group, redpanda_topic, redpanda_partition)(redpanda_kafka_consumer_group_committed_offset)
```

##  Public metrics reference

This section lists and describes each public metric, including each metric's type and labels, if available.   

### Cluster-level metrics

#### redpanda_cluster_brokers 

Number of nodes up in a cluster.

**Type**: gauge

---

#### redpanda_cluster_controller_log_limit_requests_available_rps

Limit on the available requests per second (RPS) for a cluster controller log.

**Type**: gauge

**Labels**:
- `redpanda_cmd_group=("move_operations" | "topic_operations" | "configuration_operations" | "node_management_operations" | "acls_and_users_operations")`

---

#### redpanda_cluster_controller_log_limit_requests_dropped

Number of requests dropped by a cluster controller log due to exceeding [redpanda_cluster_controller_log_limit_requests_available_rps](#redpanda_cluster_controller_log_limit_requests_available_rps).

**Type**: counter

**Labels**:
- `redpanda_cmd_group=("move_operations" | "topic_operations" | "configuration_operations" | "node_management_operations" | "acls_and_users_operations")`

---

#### redpanda_cluster_partition_moving_from_node

Number of partition replicas in the cluster that are currently being removed from a node.

**Type**: gauge

---

#### redpanda_cluster_partition_moving_to_node

Number of partition replicas in the cluster that are currently being added or moved to a node.

**Type**: gauge

---

#### redpanda_cluster_partition_node_cancelling_movements

During a partition movement cancellation operation, the number of partition replicas that were being moved that now need to be cancelled.

**Type**: gauge

---

#### redpanda_cluster_partitions

Number of partitions managed by a cluster. Includes partitions of the controller topic, but not replicas.

**Type**: gauge

---

#### redpanda_cluster_topics

Number of topics in a cluster.

**Type**: gauge

---

#### redpanda_cluster_unavailable_partitions

Number of unavailable partitions (the partitions that lack quorum among their replica group) in the cluster.

**Type**: gauge

-------------------------------------------------------------------------------

### Infrastructure-level metrics

#### redpanda_cpu_busy_seconds_total

Total CPU busy time in seconds. 

**Type**: counter

---

#### redpanda_io_queue_total_read_ops

Total read operations passed in the I/O queue.

**Type**: counter

**Labels**:
- `class=("default" | "compaction" | "raft")`
- `ioshard`
- `mountpoint`
- `shard`

---

#### redpanda_io_queue_total_write_ops

Total write operations passed in the I/O queue.

**Type**: counter

**Labels**:
- `class=("default" | "compaction" | "raft")`
- `ioshard`
- `mountpoint`
- `shard`

---

#### redpanda_memory_allocated_memory

Total allocated memory in bytes.

**Type**: gauge

**Labels**:
- `shard`

---

#### redpanda_memory_available_memory

Total memory potentially available (free plus reclaimable memory) to a CPU shard (core), in bytes.

**Type**: gauge

**Labels**:
- `shard`

---

#### redpanda_memory_available_memory_low_water_mark

The low watermark for available memory at process start.

**Type**: gauge

**Labels**:
- `shard`

---

#### redpanda_memory_free_memory

Available memory in bytes.

**Type**: gauge

**Labels**:
- `shard`

---

#### redpanda_rpc_active_connections

Number of currently active connections.

**Type**: gauge

**Labels**:
- `redpanda_server=("kafka" | "internal")`

---

#### redpanda_rpc_request_errors_total

Number of RPC errors.

**Type**: counter

**Labels**:
-  `redpanda_server=("kafka" | "internal")` 

---

#### redpanda_rpc_request_latency_seconds

RPC latency in seconds.

**Type**: histogram

**Labels**:
-  `redpanda_server=("kafka" | "internal")` 

---

#### redpanda_scheduler_runtime_seconds_total

Accumulated runtime of the task queue associated with a scheduling group.

**Type**: counter

**Labels**:
- `redpanda_scheduling_group=("admin" | "archival_upload" | "cache_background_reclaim" | "cluster" | "coproc" | "kafka" | "log_compaction" | "main" | "node_status" | "raft" | "raft_learner_recovery")`
- `shard`

---

#### redpanda_storage_disk_free_bytes

Available disk storage in bytes.

**Type**: counter

---

#### redpanda_storage_disk_free_space_alert

Alert for low disk storage: `0-OK`, `1-low space`, `2-degraded`.

**Type**: gauge

---

#### redpanda_storage_disk_total_bytes

Total size in bytes of attached storage.

**Type**: counter

---

#### redpanda_uptime_seconds_total

Total CPU runtime (uptime) in seconds.

**Type**: gauge

-------------------------------------------------------------------------------

### Node-level metrics

#### redpanda_node_status_rpcs_received

Number of node status RPCs received by a node.

**Type**: gauge

---

#### redpanda_node_status_rpcs_sent

Number of node status RPCs sent by a node.

**Type**: gauge

---

#### redpanda_node_status_rpcs_timed_out

Number of timed out node status RPCs from a node.

**Type**: gauge

-------------------------------------------------------------------------------

### Service-level metrics

#### redpanda_pandaproxy_request_latency_seconds

Latency in seconds of the request indicated by the label in HTTP Proxy. The measurement includes the time spent waiting for resources to become available, processing the request, and dispatching the response.    

**Type**: histogram

---

#### redpanda_schema_registry_request_errors_total

Total number of Schema Registry errors.

**Type**: counter

**Labels**:
- `redpanda_status=("5xx" | "4xx" | "3xx")`

---
 
#### redpanda_schema_registry_request_latency_seconds

Latency of the request indicated by the label in the Schema Registry. The measurement includes the time spent waiting for resources to become available, processing the request, and dispatching the response.

**Type**: histogram

-------------------------------------------------------------------------------

### Partition-level metrics

#### redpanda_kafka_max_offset

Offset of the last message committed for the partition.

**Type**: gauge

**Labels**:
- `redpanda_namespace` 
- `redpanda_partition` 
- `redpanda_topic` 

---

#### redpanda_kafka_under_replicated_replicas

Number of replicas in the partition that are live but not at the latest offset, [redpanda_kafka_max_offset](#redpanda_kafka_max_offset).

**Type**: gauge

**Labels**:
- `redpanda_namespace` 
- `redpanda_partition` 
- `redpanda_topic` 

---

#### redpanda_raft_recovery_partition_movement_available_bandwidth

Bandwidth available for partition movement, in bytes per sec.

**Type**: gauge

**Labels**:
- `shard`

-------------------------------------------------------------------------------

### Topic-level metrics

### redpanda_kafka_partitions

Configured number of partitions for a topic.

**Type**: gauge

**Labels**:
-`redpanda_namespace`
-`redpanda_topic`

---

### redpanda_kafka_replicas

The number of configured replicas per topic.

**Type**: gauge

**Labels**:
- `redpanda_namespace`
- `redpanda_topic`

---

#### redpanda_kafka_request_bytes_total

Total number of bytes produced or consumed per topic.

**Type**: counter

**Labels**:
- `redpanda_namespace` 
- `redpanda_topic` 
- `redpanda_request=("produce" | "consume")`

---

#### redpanda_raft_leadership_changes

Number of leadership changes across all partitions of a given topic.

**Type**: counter

**Labels**:
- `redpanda_namespace`
- `redpanda_topic`

-------------------------------------------------------------------------------

### Broker-level metrics

#### redpanda_kafka_request_latency_seconds

Latency of produce/consume requests per broker. This duration measures from when a request is initiated on the partition to when the response is fulfilled.

**Type**: histogram

**Labels**:
- `redpanda_request=("produce" | "consume")`

-------------------------------------------------------------------------------

### Consumer group metrics

#### redpanda_kafka_consumer_group_committed_offset

Committed offset of a consumer group.

**Type**: gauge

**Labels**: 
- `group`
- `topic`
- `partition`

---

#### redpanda_kafka_consumer_group_consumers

Number of consumers in a consumer group.

**Type**: gauge

**Labels**:
- `group`

---

#### redpanda_kafka_consumer_group_topics

Number of topics in a consumer group.

**Type**: gauge

**Labels**:
- `group`

-------------------------------------------------------------------------------

### REST proxy metrics

#### redpanda_rest_proxy_request_errors_total

Total number of REST proxy server errors.

**Type**: counter

**Labels**:
- `redpanda_status("5xx" | "4xx" | "3xx")`

---

#### redpanda_rest_proxy_request_latency_seconds

Internal latency of REST proxy requests. 

**Type**: histogram

-------------------------------------------------------------------------------

### Application metrics

#### redpanda_application_build

Redpanda build information.

**Type**: gauge

**Labels**:
- `redpanda_revision=<redpanda-revision-ID>`
- `redpanda_version=<redpanda-version-number>`

---

#### redpanda_application_uptime_seconds_total

Redpanda application uptime in seconds.

**Type** gauge

-------------------------------------------------------------------------------



## Next steps

Read about [Internal Metrics](../../reference/internal-metrics).