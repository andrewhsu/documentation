---
title: How Redpanda Works
---

<head>
    <meta name="title" content="Redpanda's Architecture | Redpanda Docs"/>
    <meta name="description" content="Redpanda is a distributed system that is composed of multiple nodes (servers) and data producers and consumers (clients)."/>
</head>

Redpanda provides mission-critical data infrastructure for streaming real-time data. It's fast, it's easy, and it keeps your data safe. 

## Redpanda features

### Fast
  
Redpanda is designed from the ground up for maximum performance on any data streaming workload. It can scale up to use all available resources on a single machine and scale out to distribute performance across multiple machines. This enables new use cases that require high throughput and low latency, with a minimal hardware footprint.

### Simple
  
Redpanda is packaged as a single binary. It does not rely on any external systems. Redpanda is fully compatible with the Apache Kafka© API, so it works with the ecosystem of tools and integrations that have been built on the Kafka protocol. It can be deployed in your data center or in the cloud on bare metal, containers, and virtual machines.

### Reliable
  
At its core, Redpanda is a durable and fault-tolerant transaction log for storing data streams. It uses the [Raft consensus algorithm](https://raft.github.io/) to write data to log files and replicate that data across multiple servers. Raft facilitates communication between the nodes in a Redpanda cluster to make sure that they agree on changes and remain in sync, even if a minority of them are in a failure state. This enables Redpanda to ride out partial environmental failures to deliver a predictable performance, even at high loads.

## Redpanda architecture

### Producers, consumers, and topics

Redpanda is a distributed system composed of multiple nodes (servers), and data producers and consumers (clients). Producers are client applications that write data to Redpanda in the form of events. An event is simply a record of something changing state, like a credit card transaction, new coordinates of a taxi, or someone pressing play on a TV series. Consumers read these events from Redpanda for processing and forwarding on to downstream systems.

Producers and consumers interact with Redpanda using the Kafka API. To achieve high scalability, they are fully decoupled. Redpanda provides strong guarantees to producers that events are stored durably within the system, and consumers can subscribe to Redpanda and read the events asynchronously. Redpanda achieves this decoupling by organizing events into topics. Topics represent a logical grouping of events that are written to the same log. A topic can have multiple producers writing events to it and multiple consumers reading events from it.

### Tiered Storage

Redpanda [Tiered Storage](../../data-management/tiered-storage/) is a multi-tiered remote storage solution that provides the ability to archive log segments to cloud storage in near real time. Tiered Storage can be combined with local storage to provide infinite data retention and disaster recovery on a per-topic basis.

Consumers that read from more recent offsets continue to read from local storage, and consumers that read from historical offsets read from object storage, all with the same API. Consumers can read and reread events from any point within the maximum retention period, whether the events reside on local or remote storage.

Local storage is finite and managed by Redpanda’s data retention settings. The size of the provisioned storage determines how much data your topics can retain. This is typically no more than a few weeks, but it depends on the size of your cluster. 

### Partitions

Topics are scaled by sharding them into one or more partitions that are distributed between the nodes in a Redpanda cluster. Dividing the data and communication allows clients to write data to, and read data from, many nodes at the same time. When producers write to a topic, they route events to one of the topic’s partitions. Events with the same key (like a stock ticker) are always routed to the same partition, and Redpanda guarantees the order of events at the partition level. Consumers read events from a partition in the order that they were written.

A single topic can have two partitions on different nodes, and the producers write to both partitions, routing events by key. Different keys can be routed to the same partition, but it is guaranteed that events with the same key are persisted in the order they were written. If a key is not specified, then events are sent to all topic-partitions in a round-robin fashion.

### Raft consensus algorithm

Redpanda provides strong guarantees for data safety and fault tolerance. Events written to a topic-partition are appended to a log file on disk. They can be replicated to other nodes in the cluster and appended to their copies of the log file on disk to prevent data loss in the event of failure. The [Raft consensus algorithm](https://raft.github.io/) is used for data replication. 

Every topic-partition forms a Raft group consisting of a single elected leader and zero or more followers (as specified by the topic’s replication factor). A Raft group can tolerate ƒ failures given 2ƒ+1 nodes. For example, in a cluster with five nodes and a topic with a replication factor of five, the topic remains fully operational if two nodes fail.

Raft is a majority vote algorithm. For a leader to acknowledge that an event has been committed to a partition, a majority of its replicas must have written that event to their copy of the log. When a majority (quorum) of responses have been received, the leader can make the event available to consumers and respond to the producer to acknowledge receipt of the event.

As long as the leader and a majority of the replicas are stable, Redpanda can tolerate disturbances in a minority of the replicas. If gray failures cause a minority of replicas to respond slower than normal, then the leader does not have to wait for their responses to progress, and any additional latency is not passed on to the clients. The result is that Redpanda is less sensitive to faults and can deliver predictable performance.

### Partition leadership elections

Raft uses a heartbeat mechanism to maintain leadership authority and to trigger leader elections. The partition leader sends a periodic heartbeat to all followers to assert its leadership in the current term (default = 150 milliseconds). A term is an arbitrary period of time that starts when a leadership election is triggered. If a follower does not receive a heartbeat over a period of time (default = 1.5 seconds), then it triggers an election to choose a new partition leader.

When this happens, the follower increments its term and votes for itself to be the leader for that term. It then sends a vote request to the other nodes and waits until one of the following:

- It receives a majority of votes and becomes the leader. Raft guarantees that at most one candidate can be elected the leader for a given term.

- Another follower establishes itself as the leader. While waiting for votes, the candidate may receive communication from another node in the group claiming to be the leader. The candidate only accepts the claim if its term is greater than or equal to the candidate's term; otherwise, the communication is rejected and the candidate continues to wait for votes.

- No leader is elected over a period of time. If multiple followers timeout and become election candidates at the same time, it's possible that no candidate gets a majority of votes. When this happens, each candidate increments its term and triggers a new election round. Raft uses a random timeout between 150-300 milliseconds to ensure that split votes are rare and resolved quickly. 

As long as there is a timing inequality between heartbeat time, election timeout, and mean time between node failures (MTBF), then Raft can elect and maintain a steady leader and make progress. A leader can maintain its position as long as one of the ten heartbeat messages it sends to all of its followers every 1.5 seconds is received; otherwise, a new leader is elected.

If a follower triggers an election, but the incumbent leader subsequently springs back to life and starts sending data again, then it’s too late. As part of the election process, the follower (now an election candidate) incremented the term and rejects requests from the previous term, essentially forcing a leadership change. If a cluster is experiencing wider network infrastructure problems that result in latencies above the heartbeat timeout, then back-to-back election rounds can be triggered. During this period, unstable Raft groups may not be able to form a quorum. This results in partitions rejecting writes, but no data loss can occur. Redpanda has a Raft-priority implementation that allows the system to settle quickly after network outages.

### Thread-per-core model

Redpanda is designed to exploit advances in modern hardware, from the network down to the disks. Network bandwidth has increased considerably, especially in the cloud, and spinning disks have been replaced by SSD devices that deliver better I/O performance. CPUs are faster too, but this is largely due to the increased core counts as opposed to the increase in single-core speeds.

Redpanda implements a thread-per-core programming model through its use of the [Seastar](https://seastar.io/) library. This allows Redpanda to pin each of its application threads to a CPU core to avoid context switching and blocking. It combines this with structured message passing (SMP) to asynchronously communicate between the pinned threads. This allows Redpanda to avoid the overhead of context switching and expensive locking operations to improve processing performance and efficiency.

From a sizing perspective, Redpanda’s ability to efficiently use all available hardware lets it scale up to get the most out of your infrastructure, before you’re forced to scale out to meet the demands of your workload. Redpanda delivers better performance with a smaller footprint, resulting in reduced operational costs and complexity.

## Next steps

- [Redpanda Licensing](../../introduction/licenses)
- [Install Redpanda](../../quickstart/)

---
## Suggested reading

- [Thread-per-core buffer management for a modern Kafka-API storage system](https://redpanda.com/blog/tpc-buffers?utm_medium=content&utm_assetname=sizing_guide&utm_assettype=report&utm_source=gated_content&utm_campaign=tpc_architecture_blog)