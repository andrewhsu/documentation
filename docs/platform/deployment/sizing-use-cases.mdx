---
title: Sizing Use Cases
---

<head>
    <meta name="title" content="Sizing Use Cases| Redpanda Docs"/>
    <meta name="description" content="How to size Redpanda clusters for low, medium, and high throughput use cases in your data center or in the cloud."/>
</head>

The following scenarios describe considerations when sizing Redpanda clusters for different throughput and retention use cases in your data center and in the cloud.

For details about sizing considerations, see [Sizing Guidelines](../../deployment/sizing).

## Low throughput, short retention

- Average message size: 250 KB
- Producer rate: 300 messages per second
- Producer throughput: 75 MB/sec (600 Mbps)
- Consumer rate: 300 messages per second
- Consumer throughput: 75 MB/sec (600 Mbps)
- Data retention: 3 days
- Failure tolerance: 1 node

This use case uses the minimum number of Redpanda nodes. Despite the relatively low throughput of 150 MB/sec (producer and consumer), it’s important to calculate the expected bandwidth utilization and to use a network testing tool like iperf to verify that the bandwidth is available and sustainable. With a single topic with a replication factor of three, producing 75 MB/sec generates an additional 150 MB/sec of data transmitted over the network for replication, and it generates a further 75 MB/sec for the consumer group. That equates to an aggregate network bandwidth of 300 MB/sec, or 2.4 Gbps in networking terms.

Three nodes satisfy Redpanda’s minimum deployment requirement (so Raft can form quorums) and also the single node failure tolerance. The throughput is not high enough to warrant any more than two cores and a single NVMe SSD disk. Be mindful of predicted growth, and estimate how much room the cluster has now and when it might need to scale up or scale out.

With an average producer throughput of 75 MB/sec and a replication factor of three, each node writes 254 GB of data each hour and 6.4 TB of data each day. To satisfy the data retention requirement of three days, each node needs at least 20 TB of storage.

The following machine specifications provide a minimum for a bare metal cluster or its cloud-based equivalent.

|  | Bare Metal | AWS | GCP | Azure |
| ----------- | ----------- | ----------- | ----------- | ----------- |
| Instance Type | - | m5.large | n2-standard-2 | F2s_v2 |
| Nodes | 3 | 3 | 3 | 3 |
| Cores | 2 | 2 | 2 | 2 |
| Memory | 4 GB | 8 GB | 8 GB | 4 GB |
| Instance Storage | 20 TB (NVMe) | - | - | 16 GB (SSD) |
| Persistent Storage | - | 20 TB 9gb3) | 20 TB (Zonal SSD PD) | 20 TB (Standard SSD) |
| Network | 4 Gbps | Up to 10 Gbps | 10 Gbps | 5 Gbps |
| Tiered Storage | False | False | False | False |

## Medium throughput, high retention

- Average message size: 250 KB
- Producer rate: 2,000 messages per second
- Producer throughput: ~500 MB/sec (~4,000 Mbps)
- Consumer rate: 4,000 messages per second
- Consumer throughput: ~1,000 MB/sec (~8,000 Mbps)
- Data retention: 1 year
- Failure tolerance: 1 node

This use case requires a much longer retention of data. Producing an average of 500 MB/sec and consuming an average of 1,000 MB/sec equates to 2,500 MB/sec (20 Gbps) of network bandwidth for replication traffic. This is attainable but expensive with cloud providers, and these speeds are not as prevalent throughout a typical data center.

With at least one partition for each core, the 500 MB/sec of data from producers is evenly distributed between the nodes. For example, with three nodes, each node receives approximately 167 MB/sec. However, that bandwidth value increases with data replication.

| Producer MB/sec | Consumer MB/sec | Avg. Replication Factor | Nodes | Writes per node MB/sec | Reads per node MB/sec |
| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| 500 | 1,500 | 3 | 3 | 500/3 * 3 = 500 | 1500/3 = 500 |
| 500 | 1,500 | 3 | 5 | 500/5 * 3 = 300 | 1500/5 = 300 |
| 500 | 1,500 | 3 | 7 | 500/7 * 3 = 215 | 1500/7 = 215 |
| 500 | 1,500 | 5 | 7 | 500/7 * 5 = 358 | 1500/7 = 215 |

The additional 500 MB/sec for consumer throughput is for Tiered Storage and the bandwidth required to archive log segments to object storage. When Tiered Storage is enabled on a topic, it essentially adds another consumer’s worth of bandwidth on the network.

Producing an average of 500 MB every second equates to over 40 TB of data each day. With a retention period of one year, the cluster needs over 14 PB of storage. This is more than can be squeezed into three nodes, so you’re forced to overprovision just to handle the data retention requirements. This isn’t an issue in the cloud with high-performance persistent storage options (like io2 in AWS, Extreme PD in GCP, and Ultra SSD in Azure), but at this scale, it’s very expensive.

A topic with Tiered Storage enabled can write to faster local storage managed by local retention settings, and at the same time, it can write the data to object storage managed by different retention settings, or left to grow indefinitely. Consumers that generally keep up with producers stream from local storage, but at this velocity that window of opportunity is narrower. The object store is a backup if a consumer needs to read from an older offset.

|  | Bare Metal | AWS | GCP | Azure |
| ----------- | ----------- | ----------- | ----------- | ----------- |
| Instance Type | - | i3en.6xlarge | n2-standard-32 | F48s_v2 |
| Nodes | 3 | 3 | 3 | 3 |
| Cores | 24 | 24 | 32 | 48 |
| Memory | 192 GB | 192 GB | 128 GB | 96 GB |
| Instance Storage | 30 TB (NVMe) | 15 TB (NVM3) | 9 TB (SSD) | 384 TB (SSD) |
| Persistent Storage | - | - | - | 20 TB (Standard SSD) |
| Network | 25 Gbps | 25 Gbps | 32 Gbps | 21 Gbps |
| Tiered Storage | True | True | True | True |

## High throughput, high retention

- Average message size: 250 KB
- Producer rate: 4,000 messages per second
- Producer throughput: 1000 MB/sec (8,000 Mbps)
- Consumer rate: 8,000 messages per second
- Consumer throughput: 2,000 MB/sec (16,000 Mbps)
- Data retention: 1 year
- Failure tolerance: 2 nodes

This use case has many topics, hundreds of partitions, and a high throughput. The combined producer and replication data equates to 8 Gbps of network traffic, plus 16 Gbps for the consumers and 8 Gbps for Tiered Storage. In total, that’s at least 32 Gbps of network bandwidth required to sustain this level of throughput. Writing at 1,000 MB/sec is near the upper limit of what a single NVMe disk can sustain.

At this scale, you get significant performance gains by distributing the writes over many cores and disks to better leverage Redpanda’s thread-per-core model. For example, given five nodes with 24 cores each, start with at least one partition for each core (120 partitions in total) and scale up.

At this scale, Redpanda generates over three TB of writes each hour and over 80 TB each day. Local storage is going to fill up quickly, and the window of opportunity for consumers to read from local storage is going to be shorter than in the other scenarios. In this use case, [Tiered Storage](../../data-management/tiered-storage/) is essential.

|  | Bare Metal | AWS | GCP | Azure |
| ----------- | ----------- | ----------- | ----------- | ----------- |
| Instance Type | - | i3en.12xlarge | n2-standard-48 | F48s_v2 |
| Nodes | 5 | 5 | 5 | 5 |
| Cores | 24 | 48 | 48 | 48 |
| Memory | 192 GB | 384 GB | 192 GB | 96 GB |
| Instance Storage | 30 TB (NVMe) | 30 TB (NVM3) | 9 TB (SSD) | 384 TB (SSD) |
| Persistent Storage | - | - | - | 30 TB (Ultra SSD) |
| Network | 25 Gbps | 25 Gbps | 32 Gbps | 21 Gbps |
| Tiered Storage | True | True | True | True |
